# Stroke Detection and Care System Using Reinforcement Learning

## Project Overview

This project implements a stroke detection and patient monitoring system using reinforcement learning (RL) and computer-vision-style rendering. A drone agent monitors a patient on a 10x10 grid and attempts to detect stroke events promptly. The repository contains training code, evaluation, visualization utilities, and scripts to compare multiple RL algorithms.

## Visualizations

The project includes comprehensive visualization outputs:

- **Simulation GIFs**: `figures/stroke_env_demo.gif` and `figures/stroke_env_simulation.gif` demonstrate the environment
- **Hyperparameter Analysis Plots**: 
  - Bar charts (`*_reward_bar.png`) showing reward distribution across 10 runs for each algorithm
  - Scatter plots (`*_scatter.png`) showing relationships between learning rate, gamma, batch size, and performance
- **Report Plots** (generated by `plot_hyperparams.py`):
  - Cumulative rewards over episodes (subplots for all methods)
  - Episodes to convergence analysis (reward per episode with moving averages)
  - Generalization testing (performance on unseen initial states)
  - Training stability (variance across hyperparameter runs)
- **Evaluation Videos**: Recorded episodes of trained agents (in `rl_agent_videos_*/` directories)

## Algorithms Implemented

This project compares four reinforcement learning algorithms:

1. **Deep Q-Network (DQN)** — Value-based method using experience replay
2. **Proximal Policy Optimization (PPO)** — Policy-gradient method with clipped objective
3. **Advantage Actor-Critic (A2C)** — On-policy actor-critic method
4. **REINFORCE** — Vanilla policy gradient method (PyTorch implementation)

Each algorithm has been trained with 10 different hyperparameter configurations to find optimal settings.

## Environment Description

The `StrokeDetectionEnv` is a custom Gymnasium environment with the following specifications:

- **Grid size**: 10×10 cells
- **Agent (Drone)**: 
  - Moves horizontally and vertically (4 directions)
  - Supports zoom actions (zoom_in, zoom_out)
  - Starts at position (5, 5)
- **Patient**: 
  - Randomly moves around the grid each timestep
  - Experiences stroke events with 5% probability per timestep
  - Position is visible in the observation space
- **Observation space**: 5-dimensional vector `[drone_x, drone_y, zoom, patient_x, patient_y]`
  - All values are floats in range [0, 9] (except zoom which ranges from 1 to 5)
- **Action space**: `Discrete(6)` — 0: left, 1: right, 2: up, 3: down, 4: zoom_in, 5: zoom_out
- **Reward structure**:
  - Base reward: -0.1 per timestep
  - +10: Detecting stroke when drone is at patient position
  - -10: Missing stroke (stroke occurs but drone is not at patient position)
  - -5: Being at patient position when no stroke occurs
- **Episode length**: Episodes terminate after 200 timesteps

## Project Structure

```
rl_summative/
├── environment/
│   ├── custom_env.py            # Custom Gymnasium environment (StrokeDetectionEnv)
│   └── rendering.py             # Visualization utilities (pygame + imageio for GIFs)
├── training/
│   ├── dqn_training.py          # Training script for DQN agent (10 hyperparameter runs)
│   ├── pg_training.py           # Training script for PPO agent (10 hyperparameter runs)
│   ├── train_a2c.py             # A2C training script (10 hyperparameter runs)
│   ├── train_reinforce.py       # REINFORCE training script (PyTorch, 10 hyperparameter runs)
│   ├── CustomLogger.py          # Callback for Stable-Baselines3 logging
│   └── plot_hyperparams.py      # Script to generate performance plots from CSV logs
├── models/                      # Saved model runs per algorithm
│   ├── dqn/run_1/ ... run_10/   # DQN models (Stable-Baselines3 format)
│   ├── pg/run_1/ ... run_10/    # PPO models (Stable-Baselines3 format)
│   ├── a2c/run_1/ ... run_10/   # A2C models (Stable-Baselines3 format)
│   └── reinforce/run_1/ ... run_10/  # REINFORCE models (PyTorch state dicts)
├── logs/                        # Hyperparameter tuning results
│   ├── dqn_hyperparams.csv
│   ├── ppo_hyperparams.csv
│   ├── a2c_hyperparams.csv
│   └── reinforce_hyperparams.csv
├── figures/                     # Generated visualizations
│   ├── *_reward_bar.png         # Bar charts for each algorithm (hyperparameter analysis)
│   ├── *_scatter.png            # Scatter plots (learning rate vs reward)
│   ├── cumulative.reward.png    # Cumulative rewards subplots (all methods)
│   ├── episode to converge..png # Convergence analysis subplots
│   ├── generalisation.test.png  # Generalization testing results
│   ├── traning,satbility.png    # Training stability (variance analysis)
│   ├── stroke_env_*.gif         # Environment simulation GIFs
│   └── static.png               # Static visualization
├── rl_agent_videos_*/           # Evaluation videos for each algorithm
│   └── rl-video-episode-*.mp4
├── main.py                      # Evaluate best models and record videos
├── requirements.txt             # Python dependencies
└── README.md                    # Project documentation
```

## Training Details

### Hyperparameter Tuning

Each algorithm was trained with 10 different hyperparameter configurations:

- **DQN**: Learning rates (5e-5 to 1e-3), gamma (0.90-0.999), batch sizes (32-256), buffer sizes (20k-150k), exploration schedules
- **PPO**: Learning rates (5e-5 to 7e-4), gamma (0.95-0.99), n_steps (512-4096), batch sizes (16-128)
- **A2C**: Learning rates (1e-4 to 1e-3), gamma (0.95-0.99), n_steps (5-20), entropy coefficients, value function coefficients
- **REINFORCE**: Learning rates (2e-4 to 2e-3), gamma (0.90-0.995), hidden sizes (64-256), episode counts (500-1200)

All algorithms were trained for 50,000 timesteps (except REINFORCE which uses episode-based training).

### Training Scripts

Each training script:
1. Creates the environment and sets up logging
2. Trains 10 model variants with different hyperparameters
3. Saves models to `models/{algorithm}/run_{1-10}/`
4. Logs hyperparameters and performance metrics to `logs/{algorithm}_hyperparams.csv`

## Evaluation

The `main.py` script:
1. Reads hyperparameter logs from `logs/*_hyperparams.csv`
2. Selects the best run for each algorithm (highest reward)
3. Loads the corresponding trained models
4. Evaluates each model for 3 episodes with video recording
5. Saves videos to `rl_agent_videos_{algorithm}/` directories

## Visualization

All visualizations have been generated and are available in the `figures/` directory. The `plot_hyperparams.py` script can regenerate them if needed:

```bash
python training/plot_hyperparams.py
```

### Available Visualizations:

1. **Hyperparameter Analysis Plots** (8 files):
   - Bar charts (`*_reward_bar.png`) comparing rewards across 10 hyperparameter runs for each algorithm
   - Scatter plots (`*_scatter.png`) showing learning rate vs reward (colored by gamma, sized by batch size)

2. **Report-Specific Plots** (4 files - all generated):
   - **Cumulative Rewards** (`cumulative.reward.png`): Subplots showing cumulative rewards over 100 evaluation episodes for all best models (DQN, PPO, A2C, REINFORCE)
   - **Episodes to Converge** (`episode to converge..png`): Subplots showing reward per episode with moving averages (window=10) to analyze convergence patterns
   - **Generalization** (`generalisation.test.png`): Bar and box plots comparing performance on 20 unseen initial states with mean ± std for each algorithm
   - **Training Stability** (`traning,satbility.png`): Variance analysis across 10 hyperparameter runs showing stability of each algorithm

3. **Environment Visualizations**:
   - `stroke_env_demo.gif` and `stroke_env_simulation.gif`: GIFs demonstrating the environment with random actions

All plots are saved with 300 DPI resolution and are ready for inclusion in the report PDF.

**Other Visualizations**:
- **Environment Demo**: Run `python environment/rendering.py` to generate a demo GIF of the environment
- **Evaluation Videos**: Generated automatically when running `main.py` (saved to `rl_agent_videos_*/` directories)

## Key Findings

Based on the hyperparameter tuning experiments:

- **DQN (Run 7)**: Achieved the best total reward (-30915.0) with optimal hyperparameters (learning_rate=0.00005, gamma=0.999, buffer_size=120k). Demonstrates that careful tuning of value-based methods can yield superior performance.
- **PPO (Run 2)**: Best among actor-critic methods (-31260.0) with fast convergence and stable training. Excellent balance of performance and stability.
- **A2C (Run 6)**: Moderate performance (-31345.0), similar to PPO but slightly worse. Simpler than PPO but less sample efficient.
- **REINFORCE (Run 6)**: Best mean reward per episode (-115.0) but uses different metric. Requires more training episodes and shows higher variance.

Critical hyperparameters:
- **Learning rate**: Lower values (1e-4 to 5e-4) generally improved stability
- **Gamma (discount factor)**: Values around 0.98-0.99 balanced short-term detection and long-term planning
- **Buffer/batch sizes (DQN)**: Significantly impact stability and sample efficiency
- **n_steps (PPO/A2C)**: Affects the trade-off between sample efficiency and variance

## Future Improvements

- Integrate real-time clinical data or synthetic video streams for more realistic observation inputs
- Model more realistic drone physics and sensor noise
- Expand experiments to distributional RL (e.g., C51, QR-DQN) and model-based approaches
- Add evaluation metrics beyond episodic reward (e.g., detection latency, false-positive rate, time-to-detection)
- Implement multi-agent scenarios with multiple drones
- Add partial observability or noisy observations for more realistic conditions

## Quick Start

### 1. Installation

Create and activate a virtual environment:

```bash
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

Install dependencies:

```bash
pip install -r requirements.txt
```

### 2. Training Models

Train all algorithms (each script runs 10 hyperparameter configurations):

```bash
# Train DQN (takes ~30-60 minutes depending on hardware)
python training/dqn_training.py

# Train PPO
python training/pg_training.py

# Train A2C
python training/train_a2c.py

# Train REINFORCE
python training/train_reinforce.py
```

**Note**: Training can take significant time. Each algorithm runs 10 configurations with 50,000 timesteps each.

### 3. Generate Visualizations

Generate all performance plots and report visualizations:

```bash
python training/plot_hyperparams.py
```

This generates:
- Hyperparameter analysis plots (bar charts and scatter plots for each algorithm)
- Cumulative rewards plots (subplots for all methods)
- Episodes to converge analysis
- Generalization testing results
- Training stability plots

All plots are saved to the `figures/` directory with high resolution (300 DPI).

### 4. Evaluate Best Models

Evaluate the best model from each algorithm and record videos:

```bash
python main.py
```

This will:
- Load the best model for each algorithm (based on highest reward in CSV logs)
- Run 3 evaluation episodes per algorithm
- Save videos to `rl_agent_videos_{algorithm}/` directories

**Prerequisites**: Ensure training has been completed and `logs/*_hyperparams.csv` files exist.

### 5. Generate Environment Demo

Create a demo GIF of the environment:

```bash
python environment/rendering.py
```

## Dependencies

Key libraries:
- `gymnasium` - RL environment interface
- `stable-baselines3` - DQN, PPO, A2C implementations
- `torch` - PyTorch for REINFORCE and neural networks
- `pygame` - Rendering and visualization
- `imageio` - GIF/video generation
- `matplotlib`, `seaborn` - Plotting and analysis
- `numpy`, `pandas` - Data processing

See `requirements.txt` for complete list.

## Notes

- **Model Formats**: 
  - Stable-Baselines3 models (DQN, PPO, A2C) are saved as `.zip` files
  - REINFORCE models are saved as PyTorch state dicts (`.pt` files)
- **Evaluation**: `main.py` requires trained models and CSV logs. Run training scripts first.
- **Video Recording**: Videos are automatically recorded during evaluation using Gymnasium's `RecordVideo` wrapper
- **Hyperparameter Logs**: All training runs log hyperparameters and performance to CSV files in `logs/`

## Credits

This project was developed as educational summative project exploring reinforcement learning for early medical event detection and monitoring systems.


